{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubeflow Pipeline - MNIST Classification Example\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to our first end-to-end example! Today, we're working through a classification problem, specifically of images of handwritten digits, from zero to nine. Let's imagine that this dataset doesn't have labels, so we don't know for sure what the true answer is. In later examples, we'll show the value of \"ground truth\", as it's commonly known.\n",
    "\n",
    "Today, however, we need to get these digits classified without ground truth. A common method for doing this is a set of methods known as \"clustering\", and in particular, the method that we'll look at today is called k-means clustering. \n",
    "\n",
    "In this method, each point belongs to the cluster with the closest mean, and the data is partitioned into a number of clusters that is specified when framing the problem. In this case, since we know there are 10 clusters, and we have no labeled data (in the way we framed the problem), this is a good fit.\n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kubeflow Pipeline Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import json\n",
    "import copy\n",
    "from kfp import components\n",
    "from kfp import dsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Step\n",
    "\n",
    "We read the dataset from the existing repository into memory, for preprocessing prior to training. In this case we'll use the MNIST dataset, which contains 70K 28 x 28 pixel images of handwritten digits. For more details, please see here.\n",
    "\n",
    "This processing could be done in situ by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn't onerous, though it would be for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this to match the name of your bucket\n",
    "my_bucket_name = \"0mk7hr-eks-ml-data\"\n",
    "\n",
    "sagemaker_hpo_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/c52a73e52c64a2d1414d0294e8617da42445dfd8/components/aws/sagemaker/hyperparameter_tuning/component.yaml')\n",
    "sagemaker_process_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/c52a73e52c64a2d1414d0294e8617da42445dfd8/components/aws/sagemaker/process/component.yaml')\n",
    "sagemaker_train_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/c52a73e52c64a2d1414d0294e8617da42445dfd8/components/aws/sagemaker/train/component.yaml')\n",
    "sagemaker_model_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/c52a73e52c64a2d1414d0294e8617da42445dfd8/components/aws/sagemaker/model/component.yaml')\n",
    "sagemaker_deploy_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/c52a73e52c64a2d1414d0294e8617da42445dfd8/components/aws/sagemaker/deploy/component.yaml')\n",
    "sagemaker_batch_transform_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/c52a73e52c64a2d1414d0294e8617da42445dfd8/components/aws/sagemaker/batch_transform/component.yaml')\n",
    "\n",
    "\n",
    "def processing_input(input_name, s3_uri, local_path):\n",
    "    return {\n",
    "        \"InputName\": input_name,\n",
    "        \"S3Input\": {\n",
    "            \"S3Uri\": s3_uri,\n",
    "            \"LocalPath\": local_path,\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3InputMode\": \"File\",\n",
    "            \"S3CompressionType\": \"None\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def processing_output(output_name, s3_uri, local_path):\n",
    "    return {\n",
    "        \"OutputName\": output_name,\n",
    "        \"S3Output\": {\n",
    "            \"S3Uri\": s3_uri,\n",
    "            \"LocalPath\": local_path,\n",
    "            \"S3UploadMode\": \"EndOfJob\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def training_input(input_name, s3_uri):\n",
    "    return {\n",
    "        \"ChannelName\": input_name,\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3Uri\": s3_uri,\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            }\n",
    "        },\n",
    "        \"CompressionType\": \"None\",\n",
    "        \"RecordWrapperType\": \"None\",\n",
    "        \"InputMode\": \"File\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpoChannels = [\n",
    "    training_input(\n",
    "        \"train\", f\"s3://{my_bucket_name}/mnist_kmeans_example/input/train_data\"\n",
    "    ),\n",
    "    training_input(\n",
    "        \"test\", f\"s3://{my_bucket_name}/mnist_kmeans_example/input/test_data\"\n",
    "    ),\n",
    "]\n",
    "trainChannels = [\n",
    "    training_input(\n",
    "        \"train\", f\"s3://{my_bucket_name}/mnist_kmeans_example/input/train_data\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition Pipeline Operation2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"MNIST Classification pipeline\",\n",
    "    description=\"MNIST Classification using KMEANS in SageMaker\",\n",
    ")\n",
    "def mnist_classification(\n",
    "    region=\"us-west-2\",\n",
    "    # General component inputs\n",
    "    output_encryption_key=\"\",\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    instance_count=1,\n",
    "    volume_size=50,\n",
    "    max_run_time=3600,\n",
    "    endpoint_url=\"\",\n",
    "    network_isolation=True,\n",
    "    traffic_encryption=False,\n",
    "    role_arn=\"\",\n",
    "    # Pre-processing inputs\n",
    "    process_instance_type=\"ml.m5.large\",\n",
    "    process_instance_count=1,\n",
    "    process_entrypoint=[\"python\", \"/opt/ml/processing/code/kmeans_preprocessing.py\"],\n",
    "    process_image=\"763104351884.dkr.ecr.west-2.amazonaws.com/pytorch-training:1.5.0-cpu-py36-ubuntu16.04\",\n",
    "    process_input_config=[\n",
    "        processing_input(\n",
    "            \"mnist_tar\",\n",
    "            \"s3://sagemaker-sample-data-us-east-1/algorithms/kmeans/mnist/mnist.pkl.gz\",\n",
    "            \"/opt/ml/processing/input\",\n",
    "        ),\n",
    "        processing_input(\n",
    "            \"source_code\",\n",
    "            f\"s3://{my_bucket_name}/mnist_kmeans_example/processing_code/kmeans_preprocessing.py\",\n",
    "            \"/opt/ml/processing/code\",\n",
    "        ),\n",
    "    ],\n",
    "    process_output_config=[\n",
    "        processing_output(\n",
    "            \"train_data\",\n",
    "            f\"s3://{my_bucket_name}/mnist_kmeans_example/input/\",\n",
    "            \"/opt/ml/processing/output_train/\",\n",
    "        ),\n",
    "        processing_output(\n",
    "            \"test_data\",\n",
    "            f\"s3://{my_bucket_name}/mnist_kmeans_example/input/\",\n",
    "            \"/opt/ml/processing/output_test/\",\n",
    "        ),\n",
    "        processing_output(\n",
    "            \"valid_data\",\n",
    "            f\"s3://{my_bucket_name}/mnist_kmeans_example/input/\",\n",
    "            \"/opt/ml/processing/output_valid/\",\n",
    "        ),\n",
    "    ],\n",
    "    # HyperParameter Tuning inputs\n",
    "    hpo_strategy=\"Bayesian\",\n",
    "    hpo_metric_name=\"test:msd\",\n",
    "    hpo_metric_type=\"Minimize\",\n",
    "    hpo_early_stopping_type=\"Off\",\n",
    "    hpo_static_parameters={\"k\": \"10\", \"feature_dim\": \"784\"},\n",
    "    hpo_integer_parameters=[\n",
    "        {\"Name\": \"mini_batch_size\", \"MinValue\": \"500\", \"MaxValue\": \"600\"},\n",
    "        {\"Name\": \"extra_center_factor\", \"MinValue\": \"10\", \"MaxValue\": \"20\"},\n",
    "    ],\n",
    "    hpo_continuous_parameters=[],\n",
    "    hpo_categorical_parameters=[\n",
    "        {\"Name\": \"init_method\", \"Values\": [\"random\", \"kmeans++\"]}\n",
    "    ],\n",
    "    hpo_channels=hpoChannels,\n",
    "    hpo_spot_instance=False,\n",
    "    hpo_max_wait_time=3600,\n",
    "    hpo_checkpoint_config={},\n",
    "    hpo_max_num_jobs=9,\n",
    "    hpo_max_parallel_jobs=3,\n",
    "    # Training inputs\n",
    "    train_image=\"382416733822.dkr.ecr.us-west-2.amazonaws.com/kmeans:1\",\n",
    "    train_input_mode=\"File\",\n",
    "    train_output_location=f\"s3://{my_bucket_name}/mnist_kmeans_example/output\",\n",
    "    train_channels=trainChannels,\n",
    "    train_spot_instance=False,\n",
    "    train_max_wait_time=3600,\n",
    "    train_checkpoint_config={},\n",
    "    # Batch transform inputs\n",
    "    batch_transform_instance_type=\"ml.m4.xlarge\",\n",
    "    batch_transform_input=f\"s3://{my_bucket_name}/mnist_kmeans_example/input/valid-data.csv\",\n",
    "    batch_transform_data_type=\"S3Prefix\",\n",
    "    batch_transform_content_type=\"text/csv\",\n",
    "    batch_transform_compression_type=\"None\",\n",
    "    batch_transform_ouput=f\"s3://{my_bucket_name}/mnist_kmeans_example/output\",\n",
    "    batch_transform_max_concurrent=4,\n",
    "    batch_transform_max_payload=6,\n",
    "    batch_strategy=\"MultiRecord\",\n",
    "    batch_transform_split_type=\"Line\",\n",
    "):\n",
    "    process = sagemaker_process_op(\n",
    "        role=role_arn,\n",
    "        region=region,\n",
    "        image=process_image,\n",
    "        instance_type=instance_type,\n",
    "        instance_count=process_instance_count,\n",
    "        volume_size=volume_size,\n",
    "        max_run_time=max_run_time,\n",
    "        container_entrypoint=process_entrypoint,\n",
    "        input_config=process_input_config,\n",
    "        output_config=process_output_config\n",
    "    )\n",
    "\n",
    "    hpo = sagemaker_hpo_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        image=train_image,\n",
    "        training_input_mode=train_input_mode,\n",
    "        strategy=hpo_strategy,\n",
    "        metric_name=hpo_metric_name,\n",
    "        metric_type=hpo_metric_type,\n",
    "        early_stopping_type=hpo_early_stopping_type,\n",
    "        static_parameters=hpo_static_parameters,\n",
    "        integer_parameters=hpo_integer_parameters,\n",
    "        continuous_parameters=hpo_continuous_parameters,\n",
    "        categorical_parameters=hpo_categorical_parameters,\n",
    "        channels=hpo_channels,\n",
    "        output_location=train_output_location,\n",
    "        output_encryption_key=output_encryption_key,\n",
    "        instance_type=instance_type,\n",
    "        instance_count=instance_count,\n",
    "        volume_size=volume_size,\n",
    "        max_num_jobs=hpo_max_num_jobs,\n",
    "        max_parallel_jobs=hpo_max_parallel_jobs,\n",
    "        max_run_time=max_run_time,\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=traffic_encryption,\n",
    "        spot_instance=hpo_spot_instance,\n",
    "        max_wait_time=hpo_max_wait_time,\n",
    "        checkpoint_config=hpo_checkpoint_config,\n",
    "        role=role_arn,\n",
    "    ).after(process)\n",
    "\n",
    "    training = sagemaker_train_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        image=train_image,\n",
    "        training_input_mode=train_input_mode,\n",
    "        hyperparameters=hpo.outputs[\"best_hyperparameters\"],\n",
    "        channels=train_channels,\n",
    "        instance_type=instance_type,\n",
    "        instance_count=instance_count,\n",
    "        volume_size=volume_size,\n",
    "        max_run_time=max_run_time,\n",
    "        model_artifact_path=train_output_location,\n",
    "        output_encryption_key=output_encryption_key,\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=traffic_encryption,\n",
    "        spot_instance=train_spot_instance,\n",
    "        max_wait_time=train_max_wait_time,\n",
    "        checkpoint_config=train_checkpoint_config,\n",
    "        role=role_arn,\n",
    "    )\n",
    "\n",
    "    create_model = sagemaker_model_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        model_name=training.outputs[\"job_name\"],\n",
    "        image=training.outputs[\"training_image\"],\n",
    "        model_artifact_url=training.outputs[\"model_artifact_url\"],\n",
    "        network_isolation=network_isolation,\n",
    "        role=role_arn,\n",
    "    )\n",
    "\n",
    "    prediction = sagemaker_deploy_op(\n",
    "        region=region, endpoint_url=endpoint_url, model_name_1=create_model.output,\n",
    "    )\n",
    "\n",
    "    batch_transform = sagemaker_batch_transform_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        model_name=create_model.output,\n",
    "        instance_type=batch_transform_instance_type,\n",
    "        instance_count=instance_count,\n",
    "        max_concurrent=batch_transform_max_concurrent,\n",
    "        max_payload=batch_transform_max_payload,\n",
    "        batch_strategy=batch_strategy,\n",
    "        input_location=batch_transform_input,\n",
    "        data_type=batch_transform_data_type,\n",
    "        content_type=batch_transform_content_type,\n",
    "        split_type=batch_transform_split_type,\n",
    "        compression_type=batch_transform_compression_type,\n",
    "        output_location=batch_transform_ouput,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    kfp.compiler.Compiler().compile(mnist_classification, \"mnist-classification-pipeline.tar.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
